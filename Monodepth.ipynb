{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "from main_monodepth_pytorch import Model\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_parameters = edict({'data_dir':'data/kitti/train/',\n",
    "                         'val_data_dir':'data/kitti/val/',\n",
    "                         'model_path':'data/models/monodepth_resnet18_001.pth',\n",
    "                         'output_directory':'data/output/',\n",
    "                         'input_height':256,\n",
    "                         'input_width':512,\n",
    "                         'model':'resnet18_md',\n",
    "                         'pretrained':True,\n",
    "                         'mode':'train',\n",
    "                         'epochs':200,\n",
    "                         'learning_rate':1e-4,\n",
    "                         'batch_size': 8,\n",
    "                         'adjust_lr':True,\n",
    "                         'device':'cuda:0',\n",
    "                         'do_augmentation':True,\n",
    "                         'augment_parameters':[0.8, 1.2, 0.5, 2.0, 0.8, 1.2],\n",
    "                         'print_images':False,\n",
    "                         'print_weights':False,\n",
    "                         'input_channels': 3,\n",
    "                         'num_workers': 8,\n",
    "                         'use_multiple_gpu': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data/kitti/val/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\VSProjects]\\MonoDepth-PyTorch\\Monodepth.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/VSProjects%5D/MonoDepth-PyTorch/Monodepth.ipynb#ch0000007?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m Model(dict_parameters)\n",
      "File \u001b[1;32md:\\VSProjects]\\MonoDepth-PyTorch\\main_monodepth_pytorch.py:140\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_function \u001b[39m=\u001b[39m MonodepthLoss(\n\u001b[0;32m    135\u001b[0m         n\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[0;32m    136\u001b[0m         SSIM_w\u001b[39m=\u001b[39m\u001b[39m0.85\u001b[39m,\n\u001b[0;32m    137\u001b[0m         disp_gradient_w\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, lr_w\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters(),\n\u001b[0;32m    139\u001b[0m                                 lr\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mlearning_rate)\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_n_img, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_loader \u001b[39m=\u001b[39m prepare_dataloader(args\u001b[39m.\u001b[39;49mval_data_dir, args\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    141\u001b[0m                                                          args\u001b[39m.\u001b[39;49maugment_parameters,\n\u001b[0;32m    142\u001b[0m                                                          \u001b[39mFalse\u001b[39;49;00m, args\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[0;32m    143\u001b[0m                                                          (args\u001b[39m.\u001b[39;49minput_height, args\u001b[39m.\u001b[39;49minput_width),\n\u001b[0;32m    144\u001b[0m                                                          args\u001b[39m.\u001b[39;49mnum_workers)\n\u001b[0;32m    145\u001b[0m \u001b[39melif\u001b[39;00m args\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(args\u001b[39m.\u001b[39mmodel_path))\n",
      "File \u001b[1;32md:\\VSProjects]\\MonoDepth-PyTorch\\utils.py:55\u001b[0m, in \u001b[0;36mprepare_dataloader\u001b[1;34m(data_directory, mode, augment_parameters, do_augmentation, batch_size, size, num_workers)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_dataloader\u001b[39m(data_directory, mode, augment_parameters,\n\u001b[0;32m     54\u001b[0m                        do_augmentation, batch_size, size, num_workers):\n\u001b[1;32m---> 55\u001b[0m     data_dirs \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(data_directory)\n\u001b[0;32m     56\u001b[0m     data_transform \u001b[39m=\u001b[39m image_transforms(\n\u001b[0;32m     57\u001b[0m         mode\u001b[39m=\u001b[39mmode,\n\u001b[0;32m     58\u001b[0m         augment_parameters\u001b[39m=\u001b[39maugment_parameters,\n\u001b[0;32m     59\u001b[0m         do_augmentation\u001b[39m=\u001b[39mdo_augmentation,\n\u001b[0;32m     60\u001b[0m         size \u001b[39m=\u001b[39m size)\n\u001b[0;32m     61\u001b[0m     datasets \u001b[39m=\u001b[39m [KittiLoader(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_directory,\n\u001b[0;32m     62\u001b[0m                             data_dir), mode, transform\u001b[39m=\u001b[39mdata_transform)\n\u001b[0;32m     63\u001b[0m                             \u001b[39mfor\u001b[39;00m data_dir \u001b[39min\u001b[39;00m data_dirs]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'data/kitti/val/'"
     ]
    }
   ],
   "source": [
    "model = Model(dict_parameters)\n",
    "#model.load('data/models/monodepth_resnet18_001_last.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/models/monodepth_resnet18_001_cpt.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\VSProjects]\\MonoDepth-PyTorch\\Monodepth.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/VSProjects%5D/MonoDepth-PyTorch/Monodepth.ipynb#ch0000010?line=0'>1</a>\u001b[0m dict_parameters_test \u001b[39m=\u001b[39m edict({\u001b[39m'\u001b[39m\u001b[39mdata_dir\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mdata/test\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/VSProjects%5D/MonoDepth-PyTorch/Monodepth.ipynb#ch0000010?line=1'>2</a>\u001b[0m                               \u001b[39m'\u001b[39m\u001b[39mmodel_path\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mdata/models/monodepth_resnet18_001_cpt.pth\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/VSProjects%5D/MonoDepth-PyTorch/Monodepth.ipynb#ch0000010?line=2'>3</a>\u001b[0m                               \u001b[39m'\u001b[39m\u001b[39moutput_directory\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mdata/output/\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VSProjects%5D/MonoDepth-PyTorch/Monodepth.ipynb#ch0000010?line=10'>11</a>\u001b[0m                               \u001b[39m'\u001b[39m\u001b[39mnum_workers\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m4\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VSProjects%5D/MonoDepth-PyTorch/Monodepth.ipynb#ch0000010?line=11'>12</a>\u001b[0m                               \u001b[39m'\u001b[39m\u001b[39muse_multiple_gpu\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mFalse\u001b[39;00m})\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/VSProjects%5D/MonoDepth-PyTorch/Monodepth.ipynb#ch0000010?line=12'>13</a>\u001b[0m model_test \u001b[39m=\u001b[39m Model(dict_parameters_test)\n",
      "File \u001b[1;32md:\\VSProjects]\\MonoDepth-PyTorch\\main_monodepth_pytorch.py:146\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_n_img, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_loader \u001b[39m=\u001b[39m prepare_dataloader(args\u001b[39m.\u001b[39mval_data_dir, args\u001b[39m.\u001b[39mmode,\n\u001b[0;32m    141\u001b[0m                                                          args\u001b[39m.\u001b[39maugment_parameters,\n\u001b[0;32m    142\u001b[0m                                                          \u001b[39mFalse\u001b[39;00m, args\u001b[39m.\u001b[39mbatch_size,\n\u001b[0;32m    143\u001b[0m                                                          (args\u001b[39m.\u001b[39minput_height, args\u001b[39m.\u001b[39minput_width),\n\u001b[0;32m    144\u001b[0m                                                          args\u001b[39m.\u001b[39mnum_workers)\n\u001b[0;32m    145\u001b[0m \u001b[39melif\u001b[39;00m args\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(args\u001b[39m.\u001b[39;49mmodel_path))\n\u001b[0;32m    147\u001b[0m     args\u001b[39m.\u001b[39maugment_parameters \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    148\u001b[0m     args\u001b[39m.\u001b[39mdo_augmentation \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\depth_torch\\lib\\site-packages\\torch\\serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    697\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 699\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    701\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    703\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\depth_torch\\lib\\site-packages\\torch\\serialization.py:231\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    230\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 231\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    232\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\depth_torch\\lib\\site-packages\\torch\\serialization.py:212\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 212\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/models/monodepth_resnet18_001_cpt.pth'"
     ]
    }
   ],
   "source": [
    "dict_parameters_test = edict({'data_dir':'data/test',\n",
    "                              'model_path':'data/models/monodepth_resnet18_001_cpt.pth',\n",
    "                              'output_directory':'data/output/',\n",
    "                              'input_height':256,\n",
    "                              'input_width':512,\n",
    "                              'model':'resnet18_md',\n",
    "                              'pretrained':False,\n",
    "                              'mode':'test',\n",
    "                              'device':'cuda:0',\n",
    "                              'input_channels':3,\n",
    "                              'num_workers':4,\n",
    "                              'use_multiple_gpu':False})\n",
    "model_test = Model(dict_parameters_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = np.load('data/output/disparities_pp.npy')  # Or disparities.npy for output without post-processing\n",
    "disp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_to_img = skimage.transform.resize(disp[0].squeeze(), [375, 1242], mode='constant')\n",
    "plt.imshow(disp_to_img, cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a color image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave(os.path.join(dict_parameters_test.output_directory,\n",
    "                        dict_parameters_test.model_path.split('/')[-1][:-4]+'_test_output.png'), disp_to_img, cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(disp.shape[0]):\n",
    "    disp_to_img = skimage.transform.resize(disp[i].squeeze(), [375, 1242], mode='constant')\n",
    "    plt.imsave(os.path.join(dict_parameters_test.output_directory,\n",
    "               'pred_'+str(i)+'.png'), disp_to_img, cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a grayscale image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave(os.path.join(dict_parameters_test.output_directory,\n",
    "                        dict_parameters_test.model_path.split('/')[-1][:-4]+'_gray.png'), disp_to_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('depth_torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d2287adde87c3e1e819d14c9cacf0cc2fac52cc04e63e2aba47ded421d2da5a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
